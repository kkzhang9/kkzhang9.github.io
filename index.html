<html>
<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Kun Zhang (张坤)</title>
  <meta content="Kun Zhang (张坤), kkzhang9.github.io" name="keywords" />
  <style media="screen" type="text/css">
  html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  /* font-family: Lato, sans-serif; */
  font-family: Arial;
  /* font-family: Georgia; */
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #043d98;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}



* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 2em auto 2em auto;
  width: 870px;
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 15px;
  background: #F4F6F6;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  /* font-family: Arial, Verdana, Helvetica, sans-serif; */
  /* font-size: 13px; */
  font-weight:bold;
}

ul { 
  /* list-style: circle; */
  list-style: disc;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Arial, Helvetica, sans-serif;
  font-size: 14px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
  font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.2em;
  background: #F4F6F6;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 2px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}
div.paper2 {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

div.experience {
  /* clear: both; */
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  /* border: 2px solid #ddd; */
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  /* border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px; */
  line-height: 140%;
}


div.education {
  /* clear: both; */
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  /* border: 2px solid #ddd; */
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  /* border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px; */
  line-height: 140%;
}


div.paper:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}

div.paper2:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}
div.bio {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 135%;
}

div.res {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.award {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.paper div {
  padding-left: 270px;
}

div.experience div {
  padding-left: 200px;
}

div.education div {
  padding-left: 200px;
}

img.paper {
  /* margin-bottom: 0.4em; */
  float: left;
  width: 250px;

}

img.experience {
  /* margin-bottom: 0.4em; */
  padding-top: 10px;
  float: left;
  width: 170px;

}

img.education {
  /* margin-bottom: 0.4em; */
  padding-left: 30px;
  float: left;
  width: 100px;

}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 14px;
  margin: 1em 0;
  padding: 0;
}

    .bot {
  font-size: 14%;
}

   .ptypej {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #5cb85c;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypec {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #428bca;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypep {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #6B6B6B;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
/* navigation */
#nav {
  /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',
       Corbel, Arial, Helvetica, sans-serif; */
  font-family: Georgia, Helvetica, sans-serif;
  position: fixed;
  top: 50px;
  /* left: 860px; */
  margin-left: 860px;     /*1060*/
  width: 92px;
  font-size: 15px;
}

#nav li2 {
    margin-bottom: 1px;
}
ol {
  list-style: none;
}
#nav a {
    display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none;
}

#nav a:hover {
    color: #ffde00;
    /* background-color: #242d36 ; */
}
</style>

<!-- <script type="text/javascript" async="" src="./files/ga.js"></script>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function () {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script> -->

<script type="text/javascript" src="./files/hidebib.js"></script>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

 <!--<script src="./files/main.js"></script> -->

 <!--<li><a href="#news" title="News">News</a></li> -->

<body>
  <ol id="nav">
    <li><a href="#home" title="Home">Home</a></li>
    <li><a href="#pub" title="Papers">Papers</a></li>
    <li><a href="mailto:kkzhang@ustc.edu.cn" title="Contact">Contact</a></li>
  </ol>
<a name="home"></a>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Kun Zhang (张坤)" style="float: left; padding-left: .01em; height: 140px;" src="./fuji/kkzhang1.png" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Kun Zhang (张坤)</span><br />
<!-- <p>&nbsp;</p> -->
<p><a href="https://scholar.google.com.hk/citations?user=v1Y0NCQAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
  <a href="https://github.com/kkzhang95">Github</a> &nbsp/&nbsp
  <a href="https://www.cnblogs.com/lemonzhang">Blog</a> &nbsp/&nbsp
<!--   <a href="https://www.linkedin.com/in/%E6%96%87%E7%81%8F-%E5%90%B4-aab583128/?locale=en_US">Linkedin</a> &nbsp/&nbsp -->
  <a href="https://www.researchgate.net/profile/Zhang-Kun-32">Researchgate</a></p>
<p>&nbsp;</p>




<p><strong>Postdoctoral Researcher, <a href="https://www.ustc.edu.cn/">University of Science and Technology of China (USTC)</a></strong><br />
<!-- <p><strong>Ph.D. Candidate, University of Science and Technology of China</strong><br /><br />
 --><!-- <p><strong>Senior R&D Engineer @ Baidu Inc.</strong><br /><br /> -->
  
<span><strong>School Email</strong>: kkzhang (at) ustc.edu.cn</span> <br />
<span><strong>Motto</strong>: Curiosity, Thirst for knowledge, Understanding, Creativity</span> <br />
<!--<span><strong>Personal Email </strong>: xxxx (at) foxmail.com</span> <br /> -->
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<!-- <h2>(<a href='https://scholar.google.com/citations?user=kWADCMUAAAAJ&hl=zh-CN'>Google scholar</a>)</h2> -->
<h2>About Me</h2>
<div class="bio">



Currently, I am a Postdoctoral Researcher at Suzhou Institute for Advanced Research, University of Science and Technology of China (USTC), advised by Prof. <a href="https://scholar.google.com/citations?user=8eNm2GMAAAAJ&hl=en">S. Kevin Zhou</a> (IEEE Fellow) and Prof. <a href="https://scholar.google.com/citations?user=7sFMIKoAAAAJ&hl=zh-CN">Houqiang Li</a> (IEEE Fellow). Before that, I obtained my Ph.D. degree from the Department of Electronic Engineering and Information Science, University of Science and Technology of China (USTC) in 2024, advised by Prof. <a href="https://scholar.google.com.hk/citations?user=hxGs4ukAAAAJ&hl=zh-CN">Yongdong Zhang</a> and Prof. <a href="https://scholar.google.com/citations?user=m-0P8sgAAAAJ&hl=zh-CN">Zhendong Mao</a>. From 2018 to 2020, I studied in the Department of Automation at USTC, advised by Prof. <a href="https://scholar.google.com/citations?user=2oPsqNQAAAAJ&hl=zh-CN">Shuang Cong</a>. I obtained my B. Eng. degree in the School of Internet of Things Engineering from Jiangnan University in 2018.



<!-- <br><br>
My research interests focus on computer vision, especially low-level vision tasks.
 -->
  
<br><br>
My research interests broadly lie in the areas of Multimodal Artificial Intelligence and Deep Learning (e.g., vision-language alignment, cross-modal retrieval, report generation, retrieval augmented generation, hallucination evaluation, etc.). I am recently interested in multimodal large language models in medical scenarios. 
<br><br>
<!--
<strong style="font-size:16px"> I am maintaining a GitHub repository named <a href="https://github.com/mdyao/Awesome-3D-AIGC/"> Awesome-3D-AIGC</a>, aiming to keep pace with the rapidly evolving 3D AIGC. If you have any additions or suggestions, feel free to contribute.</strong>
 -->
<!-- including <strong style="font-size:15px;color:#8aa371">low-level vision</strong> 
, 
<strong style="font-size:15px;color:#8aa371">Spectral image processing</strong>. -->
<!-- Recently, I have shifted my focus to 
<strong style="font-size:15px;color:#0294b9"></strong> (e.g., combining vision and language) and
<strong style="font-size:15px;color:#0294b9">AI Generated Content (AIGC)</strong>.
 -->

<!-- 
<br><br>
<strong style="font-size:16px">I will join <a href="https://www.cuhk.edu.hk/">The Chinese University of Hong Kong (CUHK)</a> as a Postdoctoral Researcher, advised by Prof. <a href="https://www.gujinwei.org/">Jinwei Gu</a> and  Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a></strong>.



<br><br> -->


</div>
</div>

<!-- 
<a name="news"></a>
<div style="clear: both;">
<div class="section">
  <h2>Updates</h2>
  <div class="paper">
    <ul>

    <li><a class="button" href="#" style="color:#FFA500"><strong style="font-size:16px">New</strong></a> <strong
        style="padding-left:5px;">09/2023: </strong>
      <em> One paper is accepted by <strong><a href="https://nips.cc/">
            <font color="DarkRed">NeurIPS 2023</font></a></strong>.
        </em>
    </li>

    <li><a class="button" href="#" style="color:#FFA500"><strong style="font-size:16px">New</strong></a> <strong
        style="padding-left:5px;">07/2023: </strong>
      <em> One paper is accepted by <strong><a href="https://www.acmmm2023.org/">
            <font color="DarkRed">ACMMM 2023</font></a></strong>.
        </em>
    </li>

    <li><a class="button" href="#" style="color:#FFA500"><strong style="font-size:16px">New</strong></a> <strong
        style="padding-left:5px;">07/2023: </strong>
      <em> One paper is accepted by <strong><a href="https://iccv2023.thecvf.com/">
            <font color="DarkRed">ICCV 2023</font></a></strong>.
        </em>
    </li>



    <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:16px">New</strong></a> <strong
        style="padding-left:5px;">02/2023: </strong>
      <em> Two papers are accepted by <strong><a href="https://cvpr2023.thecvf.com/">
            <font color="DarkRed">CVPR 2023</font></a></strong>.
        </em>
    </li>

    <li> <strong>02/2023: </strong>
      <em> One paper is accepted by  <strong><font color="DarkRed">T-CSVT</font></a></strong>.</em>
    </li>


    <li> <strong>01/2023: </strong>
      <em> One papers is accepted by
        <strong><font color="DarkRed">T-MM</font></a></strong>.</em>
    </li>

    <li> <strong>09/2022: </strong>
      <em> Our <a href="https://github.com/XrKang/NeSR">NeSR</a>, 
        the first work on implicit neural representation for spectral images, is accepted by
        <strong><a href="https://mipi-challenge.org/MIPI2022/">
            <font color="DarkRed">ECCVW 2022</font>,<font color="Gray"> Mobile Intelligent Photography & Imaging
Workshop</em></font>
          </a></strong>(<font color="Red"><b>Best Paper Honorable Mention</b></font>).</em>
    </li>


    <li> <strong>07/2022: </strong>
      <em> Three papers are accepted by 
        <strong>
            <font color="DarkRed">ACMMM</font></strong>.
        </em>
    </li>


    <li> <strong>03/2022: </strong>
      <em> One paper is accepted by 
        <strong>
            <font color="DarkRed">CVPR 2022</font></strong>.
        </em>
    </li>

    </ul>
  </div>
</div>
</div>
-->






<a name="pub"></a>
<h2 id="confpapers">Selected Publications [ <a href="https://www.researchgate.net/profile/Zhang-Kun-32">Full List</a> ] </h2>
<!--   [ <a href="http://whwu95.github.io/publication.html">Full List</a> ] </h2> -->
<!-- <b>( *Co-first Author, <sup><span lang="EN-US"
      style="mso-bidi-font-size:8pt;font-family:Wingdings;mso-ascii-font-family:'Times New Roman';mso-hansi-font-family:'Times New Roman';mso-char-type:symbol;mso-symbol-font-family:Wingdings">*</span></sup>Correspondence)</b> -->

<div class="section">
  <div class="bio">


    <!-- TIP23 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/CVPR2022.png" />
      <div>
        <a><b>Negative-Aware Attention Framework for Image-Text Matching</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Zhendong Mao, Quan Wang, Yongdong Zhang.<br />
        <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b><font color="DarkRed">(CVPR)</font></b>, 2022</i><br />
        [ <a href='https://www.researchgate.net/publication/360642414_Negative-Aware_Attention_Framework_for_Image-Text_Matching'>PDF</a> ]
        [ <a href='https://github.com/CrossmodalGroup/NAAF'>Code</a> ]
        [ <a href='https://www.cnblogs.com/lemonzhang/p/16456403.html'>Blog</a> ]

      </div>
      <div class="spanner"></div>
    </div>


    <!-- ACMMM23 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/ACMMM2023.png" />
      <div>
        <a><b>Unlocking the Power of Cross-Dimensional Semantic Dependency for Image-Text Matching</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Lei Zhang, Bo Hu, Mengxiao Zhu, Zhendong Mao.<br />
        <i>ACM International Conference on Multimedia <b><font color="DarkRed">(ACM MM)</font></b>, 2023</i><br />
        [ <a href='https://www.researchgate.net/publication/374556150_Unlocking_the_Power_of_Cross-Dimensional_Semantic_Dependency_for_Image-Text_Matching'>PDF</a> ]
        [ <a href='https://github.com/CrossmodalGroup/X-Dim'>Code</a> ]
        [ <a href='https://www.cnblogs.com/lemonzhang/p/18267991'>Blog</a> ]

      </div>
      <div class="spanner"></div>
    </div>


    <!-- ICCV23 -->
    <div class="paper" id="xxx"><img class="paper" src="papers/TMM2023.png" />
      <div>
        <a><b>Unified Adaptive Relevance Distinguishable Attention Network for Image-Text Matching</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Zhendong Mao, Anan Liu, Yongdong Zhang.<br />
        <i>IEEE Transactions on Multimedia <b><font color="DarkRed">(IEEE-TMM)</font></b>, 2023</i><br />, <b><font color="DarkRed">(ESI Highly Cited Paper)</font></b>
        [ <a href='https://www.researchgate.net/publication/357729033_Unified_Adaptive_Relevance_Distinguishable_Attention_Network_for_Image-Text_Matching'>PDF</a> ]
        [ <a href='https://www.cnblogs.com/lemonzhang/p/16507873.html'>Blog</a> ]
        [ <a href='https://github.com/CrossmodalGroup/NAAF'>Code</a> ]
        
      </div>
      <div class="spanner"></div>
    </div>

    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/TCSVT2024.png" />
      <div>
        <a><b>Enhanced Semantic Similarity Learning Framework for Image-Text Matching</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Bo Hu, Huatian Zhang, Zhe Li, Zhendong Mao.<br />
        <i> IEEE Transactions on Circuits and Systems for Video Technology <b>
            <font color="DarkRed">(IEEE-TCSVT)</font>
          </b>, 2024 </i><br />
        [ <a href='https://www.researchgate.net/publication/373318149_Enhanced_Semantic_Similarity_Learning_Framework_for_Image-Text_Matching'>PDF</a> ]
        [ <a href='https://github.com/CrossmodalGroup/ESL'>Code</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>

    


    <!-- TMM2023 -->
    <div class="paper" id="TMM2023"><img class="paper" src="papers/AAAI2024.png" />
      <div>
        <a><b>Identification of Necessary Semantic Undertakers in the Causal View for Image-Text Matching</b></a><br />
         Huatian Zhang, Lei Zhang, <u><b style="color:darkred">Kun Zhang</b></u>, Zhendong Mao.<br />
        <i>AAAI Conference on Artificial Intelligence  <b>
            <font color="DarkRed">(AAAI)</font>
          </b>, 2024 </i><br />
        [ <a href='https://www.researchgate.net/publication/379296811_Identification_of_Necessary_Semantic_Undertakers_in_the_Causal_View_for_Image-Text_Matching'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>

    <!-- TMM2023 -->
    <div class="paper" id="TMM2023"><img class="paper" src="papers/AAAI2022.png" />
      <div>
        <a><b>Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching</b></a><br />
         Huatian Zhang, Zhendong Mao, <u><b style="color:darkred">Kun Zhang</b></u>, Yongdong Zhang.<br />
        <i>AAAI Conference on Artificial Intelligence  <b>
            <font color="DarkRed">(AAAI)</font>
          </b>, 2022 </i><br />
        [ <a href='https://www.researchgate.net/publication/359209345_Show_Your_Faith_Cross-Modal_Confidence-Aware_Network_for_Image-Text_Matching'>PDF</a> ]
        [ <a href='https://www.cnblogs.com/lemonzhang/p/16500876.html'>Blog</a> ]
        [ <a href='https://github.com/CrossmodalGroup/CMCAN'>Code</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>



    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/TCSVT2024-4.png" />
      <div>
        <a><b>Cascade Semantic Prompt Alignment Network for Image Captioning</b></a><br />
        Jingyu Li, Lei Zhang, <u><b style="color:darkred">Kun Zhang</b></u>, Bo Hu, Hongtao Xie, Zhendong Mao.<br />
        <i> IEEE Transactions on Circuits and Systems for Video Technology <b>
            <font color="DarkRed">(IEEE-TCSVT)</font>
          </b>, 2024 </i><br />
        [ <a href='https://www.researchgate.net/publication/376563265_Cascade_Semantic_Prompt_Alignment_Network_for_Image_Captioning'>PDF</a> ]
        [ <a href='https://github.com/CrossmodalGroup/CSA-Net'>Code</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>


    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/TCSVT2024-2.png" />
      <div>
        <a><b>Improving Image-Text Matching with Bidirectional Consistency of Cross-Modal Alignment</b></a><br />
        Zhe Li, Lei Zhang, <u><b style="color:darkred">Kun Zhang</b></u>, Yongdong Zhang, Zhendong Mao.<br />
        <i> IEEE Transactions on Circuits and Systems for Video Technology <b>
            <font color="DarkRed">(IEEE-TCSVT)</font>
          </b>, 2024 </i><br />
        [ <a href='https://www.researchgate.net/publication/378515837_Improving_Image-Text_Matching_with_Bidirectional_Consistency_of_Cross-Modal_Alignment'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>
    
    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/TCSVT2024-3.png" />
      <div>
        <a><b>Fast, Accurate, and Lightweight Memory-Enhanced Embedding Learning Framework for Image-Text Retrieval</b></a><br />
        Zhe Li, Lei Zhang, <u><b style="color:darkred">Kun Zhang</b></u>, Yongdong Zhang, Zhendong Mao.<br />
        <i> IEEE Transactions on Circuits and Systems for Video Technology <b>
            <font color="DarkRed">(IEEE-TCSVT)</font>
          </b>, 2024 </i><br />
        [ <a href='https://www.researchgate.net/publication/377707778_Fast_Accurate_and_Lightweight_Memory-Enhanced_Embedding_Learning_Framework_for_Image-Text_Retrieval'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>


    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/COLING2024.png" />
      <div>
        <a><b>Visual-Linguistic Dependency Encoding for Image-Text Retrieval</b></a><br />
        Wenxin Guo, Lei Zhang, <u><b style="color:darkred">Kun Zhang</b></u>, Yi Liu and Zhendong Mao.<br />
        <i> Joint International Conference on Computational Linguistics, Language Resources and Evaluation Technology <b>
            <font color="DarkRed">(COLING)</font>
          </b>, 2024 </i><br />
        [ <a href='https://aclanthology.org/2024.lrec-main.1511/'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>

    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/SIGPRO2020.png" />
      <div>
        <a><b>An Efficient Online Estimation Algorithm with Measurement Noise for Time-varying Quantum States</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Shuang Cong, Kezhi Li.<br />
        <i> Signal Processing<b>
            <font color="DarkRed">(SIGPRO)</font>
          </b>, 2021 </i><br />
        [ <a href='https://www.researchgate.net/publication/347841742_An_efficient_online_estimation_algorithm_with_measurement_noise_for_time-varying_quantum_states'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>

    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/QIP2020.png" />
      <div>
        <a><b>An Online Optimization Algorithm for Real-time Quantum State Tomography</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Shuang Cong, Kezhi Li, Tao Wang.<br />
        <i> Quantum Information Processing<b>
            <font color="DarkRed">(QIP)</font>
          </b>, 2020 </i><br />
        [ <a href='https://www.researchgate.net/publication/345453021_An_online_optimization_algorithm_for_the_real-time_quantum_state_tomography'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>


    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/EUSOIP2020.png" />
      <div>
        <a><b>An Efficient Online Estimation Algorithm for Evolving Quantum States</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Shuang Cong, Yaru Tang, Nikolaos M. Freris.<br />
        <i> IEEE European Signal Processing Conference<b>
            <font color="DarkRed">(EUSIPCO)</font>
          </b>, 2020 </i><br />
        [ <a href='https://www.researchgate.net/publication/348657973_An_Efficient_Online_Estimation_Algorithm_for_Evolving_Quantum_States'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>



    <!-- CVPR23 -->
    <div class="paper" id="CVPR23"><img class="paper" src="papers/ICICIP2019.png" />
      <div>
        <a><b>Efficient and fast optimization algorithms for quantum state filtering and estimation</b></a><br />
        <u><b style="color:darkred">Kun Zhang</b></u>, Shuang Cong, Jiao Ding, Jiaojiao Zhang, Kezhi Li. Freris.<br />
        <i> International Conference on Intelligent Control and Information Processing<b>
            <font color="DarkRed">(IEEE ICICIP)</font>
          </b>, 2019 </i><br />
        [ <a href='https://www.researchgate.net/publication/339555553_Efficient_and_Fast_Optimization_Algorithms_for_Quantum_State_Filtering_and_Estimation'>PDF</a> ]
        <br />
      </div>
      <div class="spanner"></div>
    </div>



    

</div>
</div>










<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Awards</h2>
<div class="paper">
<ul>
<li><font color="Red"><b>President Award of the Chinese Academy of Sciences </b></font> (中国科学院院长奖), 2024</li>
<li><font color="Red"><b>National Scholarship for Doctoral Students </b></font> (博士生国家奖学金), 2023</li>
<li><font color="Red"><b>USTC-SZSE Doctoral Scholarship</b></font> (中国科大-深交所博士奖学金), 2022 </li>
<li><font color="Red"><b>National Scholarship for Undergraduate Students </b></font> (本科生国家奖学金), 2015</li>
<li><font color="Blue"><b>First-class Academic Scholarship of USTC</b></font>, 2018/19/21/23</li>
<li><font color="Blue"><b>1st Place</b></font> of Wuxi Internet of Things Maker Competition, 2018</li>
<li><font color="Blue"><b>3rd Place</b></font> of China Artificial Intelligence Society Bo Er Cup Competition, 2018</li>


</ul>
<div class="spanner"></div>
</div>
</div>
</div>







<!-- <div style="clear: both;">
<div class="section"><h2>Awards</h2>
<div class="paper">
<h3>PhD:</h3>
<li><a href="https://good-design.org/projects/curbyit/">Australian Good Design Award</a></li>
<li> Faculty of Engineering Research Scholarship at the University of Sydney (<b>Tuition fees offsets + $37,207 p.a.</b>)</li>
<h3>Master:</h3>
<li> Baidu Best Newcomer, 2021</li>
<li> Excellent Student Cadre of University of Chinese Academy of Sciences, 2020
<li> Baidu Best Intern, 2019 </li>
<li> Scholarship for Academic Excellence of Shenzhen Institutes of Advanced Technology, CAS, 2018 </li>
<li> University of Chinese Academy of Sciences (UCAS) Scholarships (<b>16,000 RMB p.a.</b>), 2017-2020</li>
<h3>Undergraduate:</h3>
<li> Excellent Undergraduate Student of Central South University, 2017 </li>
<li> Outstanding Student of Central South University, 2016 </li>
<li> National Endeavor Scholarship (Top 5%), 2016 </li>
<li> Excellent National College Students Innovation and Entrepreneurship Project (20,000 RMB), 2016</li>
<li> Excellent Student Cadre of Central South University, 2015</li>
<li> Excellent League Member of Central South University, 2015</li>
<li> Scholarship for Academic Excellence of Central South University, 2014/2015/2016</li>

</div>
</div>
</div> -->

  
<div style="clear: both;">
<div class="section"><h2>Academic Activities</h2>
<div class="paper">
<h3>Reviewer</h3>
<li>Served as reviewer for many conferences or journals, including CVPR, ICCV, AAAI, ACM MM, IEEE-TMM, IEEE-TCSVT, etc.</li>

<!-- <h3>Member of IEEE, ACM, AAAI and CVF</h3> -->

</div>
</div>
</div>  


<div style="clear: both;">
<div class="section"><h2>Snapping life's beautiful moments!</h2>
<div class="paper">

<table>
  <tr>
    <td><img src="photos/ts1.png" width="200" height="200"></td>
    <td><img src="photos/ts2.png" width="200" height="200"></td>
    <td><img src="photos/ts3.png" width="400" height="200"></td>
  </tr>
</table>



</div>
</div>
</div>  




  
<!-- 
<div style="clear: both;">
  <div class="section">
    <h2>Current/Past Mentoring</h2>
    <div class="paper">
      <a href=''>Yuxiang Zhao</a> (Peking University),

    </div>
  </div>
</div>

<div style="clear: both;">
<div class="section"><h2>Collaborators & Friends</h2>
<div class="paper">
<a href=''>Chang Liu</a> (Tsinghua University),
 -->





</div>
</div>
</div>


<div style="clear:both;">
<p align="right"><font size="5">Last Updated in July, 2024</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<!-- <div id="clustrmaps-widget"></div><script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=7878ad&w=268&t=n&d=Oiyd_ZsSxaiYwrp_EYx0zILMmpUPf8-9re9sJssfxww&co=ccdaea&ct=808080&cmo=3acc3a&cmn=ff5353'></script>  -->
<!-- <div id="clustrmaps-widget"></div><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=70XgpvN3bMHdSlnCHJjHQ46uUClpu3IYW5GWa9kMek4"></script> -->
<div id="clustrmaps-widget"><script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=70XgpvN3bMHdSlnCHJjHQ46uUClpu3IYW5GWa9kMek4&cl=ffffff&w=a"></script>

</body>
</html>
